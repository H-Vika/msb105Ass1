% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  british,
  a4paper,
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage[english]{selnolig} % disable illegal ligatures
\fi


\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Is reproducibility good enough?},
  pdfauthor={Harald Bjarne Vika},
  pdflang={en-GB},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Is reproducibility good enough?}
\author{Harald Bjarne Vika}
\date{18,09,2025}
\begin{document}
\maketitle


\section{Introduction}\label{introduction}

The advancement of science relies heavily on trust in the discovery of
new data. Reproducibility plays a key role in building this trust, as it
allows scientist to verify and gain confidence in the conclusions drawn
form research. However, in recent times, the scientific community has
raised concerns about the growing number of peer-reviewed preclinical
studies that cannot be reproduced(McNutt, 2014).

In this paper, we will explore the concept of reproducibility in
scientific research, why it is important for building trust and whether
current practices are sufficient to ensure its reliability for future
scientists. We begin by reviewing the literature to clarify the
definitions and distinctions between ``reproducibility'' and ''
replicability'' and their roles in the scientific process. Next, we will
discuss whether reproduccibility is essential or if replication alone
can be considered adequate. We will also examine hoew tools such as R
and Quarto documents can enhance research reproducibility, identify
common challenges in this process, and consider possible solutions.
Finally, we will conclude by reflection on these issues and sharing our
perspective on the state and future of reproducibility in scientific
data.

\section{Literature review}\label{literature-review}

\section{Theory on reproducibility}\label{theory-on-reproducibility}

THe cover story of The Economist, titled ``How Science Goes Wrong'',
adopts the terminology for reproducibility from (Barba, n.d.), defining
it as the ability to regenerate results using the original researcher's
data and code. In contrast, the same source describes replicability as
either the process of collecting new data to arrive at the same
scientific findings as a previous study, or when an independent research
team reproduces the results using the original author's materials. These
definitions highlight that while reproducibility and replicability are
closely related and sometimes overlapping, they refer to distinct
aspects of scientific validation.

Another important concept is generalizability, which refers to the
extent to which the results of a study can be applied to different
contexts or populations beyond the original study setting. Together,
reproducibility, replicability and generalizability contribute to what
is often described as robust and reliable science, a foundation
essential for scientific progress and the ability to build confidently
on prior research.

In this paper, we will focus primarily on the role of reproducibility in
scientific data and its importance in maintaining the integrity and
trustworthiness of research.

Reproducibility is a necessary, but not sufficient, condition for
replicability. As noted by Peng (2011), reproducibility should be
considered a minimum requirement for scientific publications. As
previously discussed, reproducibility helps scientists build confidence
and trust in their findings---but what exactly does ``research
reproducibility'' mean?

Goodman et al. (2016) outlines three distinct dimensions of research
reproducibility:

\begin{itemize}
\tightlist
\item
  \textbf{Methods reproducibility} -- This refers to providing
  sufficient detail about the study's procedures and data so that the
  same research can be repeated, both theoretically and in practice.
\item
  \textbf{Results reproducibility} (also referred to as
  \emph{replicability}) -- This involves obtaining the same results as a
  previous study when conducting an independent study using the same
  methodology.
\item
  \textbf{Robustness and generalizability} -- While sometimes used
  interchangeably with reproducibility, these terms highlight additional
  aspects of scientific reliability. \emph{Robustness} refers to the
  stability of experimental conclusions under variations in baseline
  assumptions or procedures. \emph{Generalizability}, on the other hand,
  refers to the extent to which a study's findings remain valid in
  different settings or populations outside the original experimental
  framework.
\end{itemize}

Together, these concepts form a broader understanding of what it means
for scientific research to be reliable and trustworthy.

\section{Publication bias}\label{publication-bias}

When discussing reproducibility in scientific research, one major
challenge is \emph{publication bias}, which can significantly affect the
ability to publish findings. This bias can lead to a disproportionate
representation of positive results in the literature, while studies
reporting null or negative results are often left unpublished.

One serious consequence of publication bias is the increased risk of
Type I errors, false positives. A \emph{Type I error} occurs when the
null hypothesis (H₀) is incorrectly rejected, meaning we conclude that
there is an effect when, in fact, none exists. The significance level
(commonly denoted as α) represents the probability of making this kind
of error and reflects the level of risk the researcher is willing to
accept before conducting the hypothesis test (\emph{Type i and Type II
Errors}, n.d.).

If scientific journals systematically favor studies that reject the null
hypothesis, the literature may become saturated with false positives.
This situation undermines the foundation of future research, especially
when later studies attempt (and fail) to replicate the original
findings. One well-known example of this issue is the \emph{``File
Drawer Problem''} described by Rosenthal (1979), where studies that fail
to reject the null hypothesis are effectively ``filed away'' and never
published, while only statistically significant results make it to
print. This selective publication practice skews the overall body of
evidence, making false positives more likely.

The implications are far-reaching. If a prestigious journal publishes a
study with a false positive, it may discourage replication efforts due
to the perceived credibility of the source. Additionally, investing
resources into research programs based on such flawed findings can be
costly and may lead to ineffective or even harmful policy decisions.

Fields that regularly publish false positives risk losing credibility
and the trust of the scientific community (Simmons et al., 2011). As
Young et al. (2008) notes, \emph{``More alarming is the general paucity
in the literature of negative data. In some fields, almost all published
studies show formally significant results, so that statistical
significance no longer appears discriminating.''} This lack of negative
results contributes further to the distortion of the scientific record.

Publication bias also compromises meta-analyses, which aggregate data
from multiple studies to draw broader conclusions (Russo, 2007). If the
individual studies included are themselves biased toward positive
findings, the meta-analytic results will also be skewed, compounding the
problem.

In short, addressing publication bias is essential to preserving the
reliability, replicability, and credibility of scientific research.

When it comes to economics and the issue of research reproducibility,
historical efforts can be traced back to journals like
\emph{Econometrica}. In an editorial note, Frisch (1933) emphasized the
importance of transparency, stating: \emph{``Statistical and other
numerical work presented in Econometrica, the original raw data will, as
a rule, be published, unless their volume is excessive''}. This reflects
an early recognition that making data publicly available is essential
for reproducibility.

However, over time, as economic research grew in complexity and scale,
models became larger and more computationally intensive. As a result,
researchers began publishing only final results, while the raw data and
underlying code were often omitted. This shift made it increasingly
difficult if not impossible for others to replicate or reproduce
published findings.

To address this problem, various solutions were proposed. One such
initiative was the creation of data archives, such as the American
Economic Association (AEA) data repository (\emph{American {Economic
Association}}, n.d.), aimed at preserving and sharing datasets used in
published research. However, these archives often fell short of their
goal. As McCullough et al. (2008) points out: \emph{``All the
long-standing archives at economics journals do not facilitate the
reproduction of published results. The data-only archives at the Journal
of Business and Economic Statistics and the Economic Journal fail in
part because most authors do not contribute data.''} In other words, the
mere existence of archives was insufficient---author compliance was
inconsistent, and critical components such as code and documentation
were often missing.

A more robust solution emerged in the form of computable documents
research articles that integrate text, data, and executable code in a
single, reproducible format. This approach allows other researchers to
fully reproduce the original study, including data preparation, model
estimation, and results generation.

One example of this is WaveLab, a software package designed to reproduce
figures from published wavelet research. Developed with reproducibility
in mind, WaveLab includes all the source code required to replicate
results {[}\emph{WaveLab and Reproducible Research \textbar{}
SpringerLink} (n.d.); {]}. The authors encouraged readers to inspect and
use the code, promoting what they called ``really reproducible''
research.

Gentleman (2007) reinforces this idea by emphasizing the role of
software in enabling reproducibility: \emph{``The step from
disseminating analyses via a compendium to reproducible research is a
small one. By reproducible research, we mean research papers with
accompanying software tools that allow the reader to directly reproduce
the results and employ the methods that are presented in the research
paper''} (Gentleman \& Temple Lang, 2007).

In this model, the article itself includes:

\begin{itemize}
\item
  The research text
\item
  Code to import and clean the data,
\item
  Code for model estimation and testing,
\item
  And code to generate the final results.
\end{itemize}

By submitting such materials alongside the manuscript, the research
becomes fully transparent and reproducible. This not only strengthens
the credibility of the work but also enables future researchers to build
on it more effectively.

To implement effective data archiving and reproducible research, tools
such as Sweave were originally developed to integrate code and
documentation. Sweave, the predecessor of knitr, allowed users to embed
R code within LaTeX documents to dynamically generate reports
(\emph{Sweave}, n.d.).

Today, knitr is the more widely used and advanced tool, fully supported
within RStudio. RStudio itself is an evolution of earlier tools like the
R Notebook and offers an integrated development environment (IDE) that
allows users to run code in sequentialchunks, while also enabling
individual execution for experimentation and debugging (Lander, 2017).

RStudio supports the full data science workflow: it helps structure and
clean data, perform transformations, visualize results, and build
statistical or machine learning models. One powerful feature is the use
of Quarto documents, which allow users to combine narrative text with
code in a single file. These documents can be rendered into multiple
output formats---including HTML, PDF, Word, and presentation
slides---making it easy to share fully reproducible research reports.

By using these tools, researchers can ensure that their analyses are
transparent, replicable, and accessible to others.

\section{Discussion of the research
question}\label{discussion-of-the-research-question}

Replicability \emph{should} be the norm in scientific research, and
while it presents challenges, expecting it is not too much to ask---it
is, in fact, essential for credible and trustworthy science.

Historically, even early journals like \emph{Econometrica} emphasized
the importance of transparency. As Frisch (1933) noted, raw data should
be published alongside statistical work unless prohibitively large
(Frisch, 1933). This shows that the value of replicability was
acknowledged from the beginning. However, as research became more
complex and models more data-intensive, researchers increasingly
published only results, making it nearly impossible for others to
replicate their work.

Solutions like the American Economic Association's data archive were a
step forward, but they often failed in practice because many authors did
not submit their data (McCullough et al., 2008). This highlights that
infrastructure alone isn't enough stronger cultural and institutional
expectations are needed.

More promising are tools that support computable documents, such as
knitr and Quarto in RStudio, which allow researchers to embed code,
data, and analysis directly within documents. These tools make
replicability not only possible but practical. They allow for seamless
integration of text, code, and output, which can be rendered into
shareable formats like HTML, PDF, or slides. With these modern tools,
the barriers to replicable research are significantly lower than they
once were.

In short, while replicability may have seemed burdensome in the past,
today's tools have made it far more achievable. As the cost of
irreproducible research grows---damaging trust, wasting resources, and
leading to false conclusions---it becomes increasingly clear that
replicability is not an unreasonable demand. Instead, it should be a
standard practice that protects the integrity and progress of science.

Yes, Quarto documents can significantly help with reproducibility by
allowing researchers to combine code, data, analysis, and narrative in a
single, integrated document.

As discussed, tools like Sweave and later knitr paved the way for
embedding code into research documents, enabling dynamic report
generation (\emph{Sweave}, n.d.). Today, RStudio, building on these
foundations, supports Quarto, a modern tool designed specifically for
reproducible research workflows. Quarto allows researchers to write
documents that include both the text of their analysis and the
underlying code that generates the results.

What makes Quarto especially powerful is its flexibility: researchers
can use it to generate outputs in various formats---including HTML, PDF,
Word, and slides all from the same source file. This means a single
Quarto document can serve both as a scientific report and as a
reproducibility package. Code chunks can be run individually for testing
or sequentially for full analysis, and because the document includes
both the data-processing steps and the models, readers can see exactly
how the results were generated.

By using Quarto, researchers reduce ambiguity, eliminate manual errors,
and make it easier for others to validate, understand, and build upon
their work. It supports the broader goal of ``really reproducible
research,'' as described in efforts like WaveLab and in the principles
set out by Gentleman \& Temple Lang (2007), who emphasized that
reproducible research means papers accompanied by the actual tools
needed to regenerate results.

In short, Quarto is a practical and powerful solution for making
reproducibility not just possible, but efficient and well-integrated
into the research process.

Despite major advancements in tools and workflows that support
reproducibility, such as Quarto, knitr, and data archives, several key
problems still remain.

One major issue is incomplete data and code sharing. While journals and
organizations like the American Economic Association have created data
archives, these often fall short because many authors do not contribute
their data or fail to provide sufficient documentation or code
(McCullough et al., 2008). Without access to the exact datasets and the
code used in analysis, reproducibility becomes impossible, regardless of
available tools.

Another persistent problem is cultural and institutional inertia.
Although tools like RStudio and Quarto make reproducibility easier than
ever, many researchers still do not adopt these practices. There may be
a lack of incentives, a fear of being scooped, or simply insufficient
training. As a result, many studies continue to be published without the
necessary materials for replication, and efforts like computable
documents remain underutilized.

Additionally, technical barriers still exist for some users. While tools
like Quarto are powerful, they require familiarity with R, LaTeX,
markdown, and version control systems, all of which can be intimidating
to researchers without a computational background.

\section{Conclusion}\label{conclusion}

So how can we look at these problems?

\begin{itemize}
\tightlist
\item
  Enforce data and code availability policies: Journals and funding
  bodies should require that authors submit both datasets and code used
  for analysis as a condition for publication. This should be monitored
  and enforced, not just encouraged.
\item
  Promote and normalize the use of reproducible tools: More training and
  education should be offered to help researchers adopt tools like
  RStudio and Quarto. Universities, research institutions, and
  conferences should prioritize workshops and resources on reproducible
  workflows.
\item
  Standardize reproducible workflows: Encouraging the use of
  ``computable documents'' as a standard format---where all text, code,
  and data are embedded---can simplify expectations and promote
  consistency across disciplines.
\item
  Recognize and reward reproducible research: Researchers who go the
  extra mile to ensure their work is fully transparent and replicable
  should be recognized through citations, awards, or funding
  opportunities.
\item
  Lower technical barriers: Continued development of user-friendly
  tools, interfaces, and templates can make reproducible research more
  accessible to non-technical users.
\end{itemize}

In conclusion, while powerful solutions now exist to make
reproducibility achievable, real change requires a cultural shift one
that embraces transparency, provides proper incentives, and equips
researchers with both the tools and the training to adopt reproducible
practices across the board

\section{References}\label{references}

R version 4.5.1 (2025-06-13 ucrt) Platform: x86\_64-w64-mingw32/x64
Running under: Windows 11 x64 (build 26100) Matrix products: default
LAPACK version 3.12.1 locale: {[}1{]} C time zone: Europe/Oslo tzcode
source: internal attached base packages: {[}1{]} stats graphics
grDevices utils datasets methods base loaded via a namespace (and not
attached): {[}1{]} compiler\_4.5.1 cli\_3.6.5 rsconnect\_1.5.1
tools\_4.5.1 rstudioapi\_0.17.1 lifecycle\_1.0.4 {[}7{]} rlang\_1.1.6

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-zotero-321}
\emph{American {Economic Association}}. (n.d.).
https://www.aeaweb.org/journals/data.

\bibitem[\citeproctext]{ref-barbaTerminologiesReproducibleResearch2018}
Barba, L. A. (n.d.). \emph{Terminologies for reproducible research}.
\url{https://doi.org/10.48550/arXiv.1802.03311}

\bibitem[\citeproctext]{ref-frischEditorsNote1933}
Frisch, R. (1933). Editor's note. \emph{Econometrica}, \emph{1}(1),
1--4. \url{https://www.jstor.org/stable/1912224}

\bibitem[\citeproctext]{ref-gentleman2007b}
Gentleman, R., \& Temple Lang, D. (2007). Statistical {Analyses} and
{Reproducible Research}. \emph{Journal of Computational and Graphical
Statistics}, \emph{16}(1), 1--23.

\bibitem[\citeproctext]{ref-goodman2016b}
Goodman, S. N., Fanelli, D., \& Ioannidis, J. P. A. (2016). What does
research reproducibility mean? \emph{Science Translational Medicine},
\emph{8}(341), 341ps12--341ps12.

\bibitem[\citeproctext]{ref-landerEveryoneAdvancedAnalytics2017}
Lander, J. P. (2017). \emph{R for everyone: Advanced analytics and
graphics} (Second edition). Addison-Wesley, Pearson.

\bibitem[\citeproctext]{ref-mccullough2008a}
McCullough, B. D., McGeary, K. A., \& Harrison, T. D. (2008). Do
economics journal archives promote replicable research? \emph{Canadian
Journal of Economics/Revue Canadienne d'{é}conomique}, \emph{41}(4),
1406--1420.

\bibitem[\citeproctext]{ref-mcnutt2014}
McNutt, M. (2014). Reproducibility. \emph{Science}, \emph{343}(6168),
229--229.

\bibitem[\citeproctext]{ref-peng2011}
Peng, R. D. (2011).
\href{https://www.ncbi.nlm.nih.gov/pubmed/22144613}{Reproducible
{Research} in {Computational Science}}. \emph{Science},
\emph{334}(6060), 1226--1227.

\bibitem[\citeproctext]{ref-rosenthal1979}
Rosenthal, R. (1979). \emph{The file drawer problem and tolerance for
null results.}

\bibitem[\citeproctext]{ref-russoHowReviewMetaanalysis2007}
Russo, M. W. (2007). How to review a meta-analysis.
\emph{Gastroenterology \& Hepatology}, \emph{3}(8), 637--642.
\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3099299/}

\bibitem[\citeproctext]{ref-simmons2011}
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive
psychology: {Undisclosed} flexibility in data collection and analysis
allows presenting anything as significant. \emph{Psychological Science},
\emph{22}(11), 1359--1366.

\bibitem[\citeproctext]{ref-SweaveDynamicGeneration}
\emph{Sweave: Dynamic generation of statistical reports using literate
data analysis \textbar{} SpringerLink}. (n.d.).
\url{https://link.springer.com/chapter/10.1007/978-3-642-57489-4_89}

\bibitem[\citeproctext]{ref-TypeTypeII}
\emph{Type i and type II errors: What are they and why do they matter? -
c j smith, 2012}. (n.d.).
\url{https://journals.sagepub.com/doi/10.1258/phleb.2012.012j04}

\bibitem[\citeproctext]{ref-WaveLabReproducibleResearch}
\emph{WaveLab and reproducible research \textbar{} SpringerLink}.
(n.d.).
\url{https://link.springer.com/chapter/10.1007/978-1-4612-2544-7_5}

\bibitem[\citeproctext]{ref-young2008}
Young, N. S., Ioannidis, J. P. A., \& Al-Ubaydli, O. (2008). Why
{Current Publication Practices May Distort Science}. \emph{PLoS
Medicine}, \emph{5}(10), e201.

\end{CSLReferences}




\end{document}
