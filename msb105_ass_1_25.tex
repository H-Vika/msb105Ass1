% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  british,
  a4paper,
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage[english]{selnolig} % disable illegal ligatures
\fi


\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Is reproducibility good enough?},
  pdfauthor={Harald Bjarne Vika},
  pdflang={en-GB},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Is reproducibility good enough?}
\author{Harald Bjarne Vika}
\date{18,09,2025}
\begin{document}
\maketitle
\begin{abstract}
A very short abstract. Put the abstract text here. One or two paragraphs
summarising what follows below.
\end{abstract}


\section{Introduction}\label{introduction}

What is this paper about? What is discussed? Why is it of any
consequence?

The advancement of science relies heavily on trust in the discovery of
new data. Reproducibility plays a key role in building this trust, as it
allows scientist to verify and gain confidence in the conclusions drawn
form research. However, in recent times, the scientific community has
raised concerns about the growing number of peer-reviewed preclinical
studies that cannot be reproduced(McNutt, 2014).

In this paper, we will explore the concept of reproducibility in
scientific research, why it is important for building trust and whether
current practices are sufficient to ensure its reliability for future
scientists. We begin by reviewing the literature to clarify the
definitions and distinctions between ``reproducibility'' and ''
replicability'' and their roles in the scientific process. Next, we will
discuss whether reproduccibility is essential or if replication alone
can be considered adequate. We will also examine hoew tools such as R
and Quarto documents can enhance research reproducibility, identify
common challenges in this process, and consider possible solutions.
Finally, we will conclude by reflection on these issues and sharing our
perspective on the state and future of reproducibility in scientific
data.

\section{Literature review}\label{literature-review}

\section{Theory on reproducibility}\label{theory-on-reproducibility}

Smart stuff from others about the topic.

THe cover story of The Economist, titled ``How Science Goes Wrong'',
adopts the terminology for reproducibility from (Barba, n.d.), defining
it as the ability to regenerate results using the original researcher's
data and code. In contrast, the same source describes replicability as
either the process of collecting new data to arrive at the same
scientific findings as a previous study, or when an independent research
team reproduces the results using the original author's materials. These
definitions highlight that while reproducibility and replicability are
closely related and sometimes overlapping, they refer to distinct
aspects of scientific validation.

Another important concept is generalizability, which refers to the
extent to which the results of a study can be applied to different
contexts or populations beyond the original study setting. Together,
reproducibility, replicability and generalizability contribute to what
is often described as robust and reliable science, a foundation
essential for scientific progress and the ability to build confidently
on prior research.

In this paper, we will focus primarily on the role of reproducibility in
scientific data and its importance in maintaining the integrity and
trustworthiness of research.

Reproducibility is a necessary, but not sufficient, condition for
replicability. As noted by Peng (2011), reproducibility should be
considered a minimum requirement for scientific publications. As
previously discussed, reproducibility helps scientists build confidence
and trust in their findings---but what exactly does ``research
reproducibility'' mean?

Goodman et al. (2016) outlines three distinct dimensions of research
reproducibility:

\begin{itemize}
\tightlist
\item
  \textbf{Methods reproducibility} -- This refers to providing
  sufficient detail about the study's procedures and data so that the
  same research can be repeated, both theoretically and in practice.
\item
  \textbf{Results reproducibility} (also referred to as
  \emph{replicability}) -- This involves obtaining the same results as a
  previous study when conducting an independent study using the same
  methodology.
\item
  \textbf{Robustness and generalizability} -- While sometimes used
  interchangeably with reproducibility, these terms highlight additional
  aspects of scientific reliability. \emph{Robustness} refers to the
  stability of experimental conclusions under variations in baseline
  assumptions or procedures. \emph{Generalizability}, on the other hand,
  refers to the extent to which a study's findings remain valid in
  different settings or populations outside the original experimental
  framework.
\end{itemize}

Together, these concepts form a broader understanding of what it means
for scientific research to be reliable and trustworthy.

\section{Publication bias}\label{publication-bias}

When discussing reproducibility in scientific research, one major
challenge is \emph{publication bias}, which can significantly affect the
ability to publish findings. This bias can lead to a disproportionate
representation of positive results in the literature, while studies
reporting null or negative results are often left unpublished.

One serious consequence of publication bias is the increased risk of
Type I errors, false positives. A \emph{Type I error} occurs when the
null hypothesis (H₀) is incorrectly rejected, meaning we conclude that
there is an effect when, in fact, none exists. The significance level
(commonly denoted as α) represents the probability of making this kind
of error and reflects the level of risk the researcher is willing to
accept before conducting the hypothesis test (\emph{Type i and Type II
Errors}, n.d.).

If scientific journals systematically favor studies that reject the null
hypothesis, the literature may become saturated with false positives.
This situation undermines the foundation of future research, especially
when later studies attempt (and fail) to replicate the original
findings. One well-known example of this issue is the \emph{``File
Drawer Problem''} described by Rosenthal (1979), where studies that fail
to reject the null hypothesis are effectively ``filed away'' and never
published, while only statistically significant results make it to
print. This selective publication practice skews the overall body of
evidence, making false positives more likely.

The implications are far-reaching. If a prestigious journal publishes a
study with a false positive, it may discourage replication efforts due
to the perceived credibility of the source. Additionally, investing
resources into research programs based on such flawed findings can be
costly and may lead to ineffective or even harmful policy decisions.

Fields that regularly publish false positives risk losing credibility
and the trust of the scientific community (Simmons et al., 2011). As
Young et al. (2008) notes, \emph{``More alarming is the general paucity
in the literature of negative data. In some fields, almost all published
studies show formally significant results, so that statistical
significance no longer appears discriminating.''} This lack of negative
results contributes further to the distortion of the scientific record.

Publication bias also compromises meta-analyses, which aggregate data
from multiple studies to draw broader conclusions (Russo, 2007). If the
individual studies included are themselves biased toward positive
findings, the meta-analytic results will also be skewed, compounding the
problem.

In short, addressing publication bias is essential to preserving the
reliability, replicability, and credibility of scientific research.

When it comes to economics and the issue of research reproducibility,
historical efforts can be traced back to journals like
\emph{Econometrica}. In an editorial note, Frisch (1933) emphasized the
importance of transparency, stating: \emph{``Statistical and other
numerical work presented in Econometrica, the original raw data will, as
a rule, be published, unless their volume is excessive''}. This reflects
an early recognition that making data publicly available is essential
for reproducibility.

However, over time, as economic research grew in complexity and scale,
models became larger and more computationally intensive. As a result,
researchers began publishing only final results, while the raw data and
underlying code were often omitted. This shift made it increasingly
difficult---if not impossible---for others to replicate or reproduce
published findings.

To address this problem, various solutions were proposed. One such
initiative was the creation of data archives, such as the American
Economic Association (AEA) data repository (\emph{American {Economic
Association}}, n.d.), aimed at preserving and sharing datasets used in
published research. However, these archives often fell short of their
goal. As McCullough et al. (2008) points out: \emph{``All the
long-standing archives at economics journals do not facilitate the
reproduction of published results. The data-only archives at the Journal
of Business and Economic Statistics and the Economic Journal fail in
part because most authors do not contribute data.''} In other words, the
mere existence of archives was insufficient---author compliance was
inconsistent, and critical components such as code and documentation
were often missing.

A more robust solution emerged in the form of \textbf{computable
documents}---research articles that integrate text, data, and executable
code in a single, reproducible format. This approach allows other
researchers to fully reproduce the original study, including data
preparation, model estimation, and results generation.

One example of this is \textbf{WaveLab}, a software package designed to
reproduce figures from published wavelet research. Developed with
reproducibility in mind, WaveLab includes all the source code required
to replicate results {[}\emph{WaveLab and Reproducible Research
\textbar{} SpringerLink} (n.d.); {]}. The authors encouraged readers to
inspect and use the code, promoting what they called ``really
reproducible'' research.

Gentleman (2007) reinforces this idea by emphasizing the role of
software in enabling reproducibility: \emph{``The step from
disseminating analyses via a compendium to reproducible research is a
small one. By reproducible research, we mean research papers with
accompanying software tools that allow the reader to directly reproduce
the results and employ the methods that are presented in the research
paper''} (Gentleman \& Temple Lang, 2007).

In this model, the article itself includes:

\begin{itemize}
\item
  The research text
\item
  Code to import and clean the data,
\item
  Code for model estimation and testing,
\item
  And code to generate the final results.
\end{itemize}

By submitting such materials alongside the manuscript, the research
becomes fully transparent and reproducible. This not only strengthens
the credibility of the work but also enables future researchers to build
on it more effectively.

To implement effective data archiving and reproducible research, tools
such as \textbf{Sweave} were originally developed to integrate code and
documentation. Sweave, the predecessor of \textbf{knitr}, allowed users
to embed R code within LaTeX documents to dynamically generate reports
(\emph{Sweave}, n.d.).

Today, \textbf{knitr} is the more widely used and advanced tool, fully
supported within \textbf{RStudio}. RStudio itself is an evolution of
earlier tools like the R Notebook and offers an integrated development
environment (IDE) that allows users to run code in \textbf{sequential
chunks}, while also enabling individual execution for experimentation
and debugging (Lander, 2017).

RStudio supports the full data science workflow: it helps structure and
clean data, perform transformations, visualize results, and build
statistical or machine learning models. One powerful feature is the use
of \textbf{Quarto} documents, which allow users to combine narrative
text with code in a single file. These documents can be rendered into
multiple output formats---including HTML, PDF, Word, and presentation
slides---making it easy to share fully reproducible research reports.

By using these tools, researchers can ensure that their analyses are
transparent, replicable, and accessible to others.

\section{Discussion of the reseach
question}\label{discussion-of-the-reseach-question}

\begin{itemize}
\tightlist
\item
  Should replicability be the norm or is this to much to ask for now?
\item
  Can Quarto documents help with reproducibility?
\item
  What problems remains and how can these be solved?
\end{itemize}

\section{Conclusion}\label{conclusion}

\section{References}\label{references}

and

\begin{itemize}
\tightlist
\item
  Version number and reference to packages used
\item
  R version used
\end{itemize}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-zotero-321}
\emph{American {Economic Association}}. (n.d.).
https://www.aeaweb.org/journals/data.

\bibitem[\citeproctext]{ref-barbaTerminologiesReproducibleResearch2018}
Barba, L. A. (n.d.). \emph{Terminologies for reproducible research}.
\url{https://doi.org/10.48550/arXiv.1802.03311}

\bibitem[\citeproctext]{ref-frischEditorsNote1933}
Frisch, R. (1933). Editor's note. \emph{Econometrica}, \emph{1}(1),
1--4. \url{https://www.jstor.org/stable/1912224}

\bibitem[\citeproctext]{ref-gentleman2007b}
Gentleman, R., \& Temple Lang, D. (2007). Statistical {Analyses} and
{Reproducible Research}. \emph{Journal of Computational and Graphical
Statistics}, \emph{16}(1), 1--23.

\bibitem[\citeproctext]{ref-goodman2016b}
Goodman, S. N., Fanelli, D., \& Ioannidis, J. P. A. (2016). What does
research reproducibility mean? \emph{Science Translational Medicine},
\emph{8}(341), 341ps12--341ps12.

\bibitem[\citeproctext]{ref-landerEveryoneAdvancedAnalytics2017}
Lander, J. P. (2017). \emph{R for everyone: Advanced analytics and
graphics} (Second edition). Addison-Wesley, Pearson.

\bibitem[\citeproctext]{ref-mccullough2008a}
McCullough, B. D., McGeary, K. A., \& Harrison, T. D. (2008). Do
economics journal archives promote replicable research? \emph{Canadian
Journal of Economics/Revue Canadienne d'{é}conomique}, \emph{41}(4),
1406--1420.

\bibitem[\citeproctext]{ref-mcnutt2014}
McNutt, M. (2014). Reproducibility. \emph{Science}, \emph{343}(6168),
229--229.

\bibitem[\citeproctext]{ref-peng2011}
Peng, R. D. (2011).
\href{https://www.ncbi.nlm.nih.gov/pubmed/22144613}{Reproducible
{Research} in {Computational Science}}. \emph{Science},
\emph{334}(6060), 1226--1227.

\bibitem[\citeproctext]{ref-rosenthal1979}
Rosenthal, R. (1979). \emph{The file drawer problem and tolerance for
null results.}

\bibitem[\citeproctext]{ref-russoHowReviewMetaanalysis2007}
Russo, M. W. (2007). How to review a meta-analysis.
\emph{Gastroenterology \& Hepatology}, \emph{3}(8), 637--642.
\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3099299/}

\bibitem[\citeproctext]{ref-simmons2011}
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive
psychology: {Undisclosed} flexibility in data collection and analysis
allows presenting anything as significant. \emph{Psychological Science},
\emph{22}(11), 1359--1366.

\bibitem[\citeproctext]{ref-SweaveDynamicGeneration}
\emph{Sweave: Dynamic generation of statistical reports using literate
data analysis \textbar{} SpringerLink}. (n.d.).
\url{https://link.springer.com/chapter/10.1007/978-3-642-57489-4_89}

\bibitem[\citeproctext]{ref-TypeTypeII}
\emph{Type i and type II errors: What are they and why do they matter? -
c j smith, 2012}. (n.d.).
\url{https://journals.sagepub.com/doi/10.1258/phleb.2012.012j04}

\bibitem[\citeproctext]{ref-WaveLabReproducibleResearch}
\emph{WaveLab and reproducible research \textbar{} SpringerLink}.
(n.d.).
\url{https://link.springer.com/chapter/10.1007/978-1-4612-2544-7_5}

\bibitem[\citeproctext]{ref-young2008}
Young, N. S., Ioannidis, J. P. A., \& Al-Ubaydli, O. (2008). Why
{Current Publication Practices May Distort Science}. \emph{PLoS
Medicine}, \emph{5}(10), e201.

\end{CSLReferences}




\end{document}
