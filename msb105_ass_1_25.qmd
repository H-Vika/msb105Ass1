---
title: "Is reproducibility good enough?"
author: "Harald Bjarne Vika"
date: last-modified
date-format: "18,09,2025"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography: reproducibility.bib
abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
---

## Introduction

What is this paper about?
What is discussed?
Why is it of any consequence?

The advancement of science relies heavily on trust in the discovery of new data.
Reproducibility plays a key role in building this trust, as it allows scientist to verify and gain confidence in the conclusions drawn form research.
However, in recent times, the scientific community has raised concerns about the growing number of peer-reviewed preclinical studies that cannot be reproduced[@mcnutt2014].

In this paper, we will explore the concept of reproducibility in scientific research, why it is important for building trust and whether current practices are sufficient to ensure its reliability for future scientists.
We begin by reviewing the literature to clarify the definitions and distinctions between "reproducibility" and " replicability" and their roles in the scientific process.
Next, we will discuss whether reproduccibility is essential or if replication alone can be considered adequate.
We will also examine hoew tools such as R and Quarto documents can enhance research reproducibility, identify common challenges in this process, and consider possible solutions.
Finally, we will conclude by reflection on these issues and sharing our perspective on the state and future of reproducibility in scientific data.

## Literature review

## Theory on reproducibility

Smart stuff from others about the topic.

THe cover story of The Economist, titled "How Science Goes Wrong", adopts the terminology for reproducibility from [@barbaTerminologiesReproducibleResearch2018], defining it as the ability to regenerate results using the original researcher's data and code.
In contrast, the same source describes replicability as either the process of collecting new data to arrive at the same scientific findings as a previous study, or when an independent research team reproduces the results using the original author's materials.
These definitions highlight that while reproducibility and replicability are closely related and sometimes overlapping, they refer to distinct aspects of scientific validation.

Another important concept is generalizability, which refers to the extent to which the results of a study can be applied to different contexts or populations beyond the original study setting.
Together, reproducibility, replicability and generalizability contribute to what is often described as robust and reliable science, a foundation essential for scientific progress and the ability to build confidently on prior research.

In this paper, we will focus primarily on the role of reproducibility in scientific data and its importance in maintaining the integrity and trustworthiness of research.

Reproducibility is a necessary, but not sufficient, condition for replicability.
As noted by @peng2011, reproducibility should be considered a minimum requirement for scientific publications.
As previously discussed, reproducibility helps scientists build confidence and trust in their findings—but what exactly does "research reproducibility" mean?

@goodman2016b outlines three distinct dimensions of research reproducibility:

-   **Methods reproducibility** – This refers to providing sufficient detail about the study's procedures and data so that the same research can be repeated, both theoretically and in practice.
-   **Results reproducibility** (also referred to as *replicability*) – This involves obtaining the same results as a previous study when conducting an independent study using the same methodology.
-   **Robustness and generalizability** – While sometimes used interchangeably with reproducibility, these terms highlight additional aspects of scientific reliability. *Robustness* refers to the stability of experimental conclusions under variations in baseline assumptions or procedures. *Generalizability*, on the other hand, refers to the extent to which a study’s findings remain valid in different settings or populations outside the original experimental framework.

Together, these concepts form a broader understanding of what it means for scientific research to be reliable and trustworthy.

## Publication bias

When discussing reproducibility in scientific research, one major challenge is *publication bias*, which can significantly affect the ability to publish findings.
This bias can lead to a disproportionate representation of positive results in the literature, while studies reporting null or negative results are often left unpublished.

One serious consequence of publication bias is the increased risk of Type I errors, false positives.
A *Type I error* occurs when the null hypothesis (H₀) is incorrectly rejected, meaning we conclude that there is an effect when, in fact, none exists.
The significance level (commonly denoted as α) represents the probability of making this kind of error and reflects the level of risk the researcher is willing to accept before conducting the hypothesis test [@TypeTypeII].

If scientific journals systematically favor studies that reject the null hypothesis, the literature may become saturated with false positives.
This situation undermines the foundation of future research, especially when later studies attempt (and fail) to replicate the original findings.
One well-known example of this issue is the *"File Drawer Problem"* described by @rosenthal1979, where studies that fail to reject the null hypothesis are effectively "filed away" and never published, while only statistically significant results make it to print.
This selective publication practice skews the overall body of evidence, making false positives more likely.

The implications are far-reaching.
If a prestigious journal publishes a study with a false positive, it may discourage replication efforts due to the perceived credibility of the source.
Additionally, investing resources into research programs based on such flawed findings can be costly and may lead to ineffective or even harmful policy decisions.

Fields that regularly publish false positives risk losing credibility and the trust of the scientific community [@simmons2011].
As @young2008 notes, *"More alarming is the general paucity in the literature of negative data. In some fields, almost all published studies show formally significant results, so that statistical significance no longer appears discriminating."* This lack of negative results contributes further to the distortion of the scientific record.

Publication bias also compromises meta-analyses, which aggregate data from multiple studies to draw broader conclusions [@russoHowReviewMetaanalysis2007].
If the individual studies included are themselves biased toward positive findings, the meta-analytic results will also be skewed, compounding the problem.

In short, addressing publication bias is essential to preserving the reliability, replicability, and credibility of scientific research.

So when it comes it economic and how to be able to reproduce the research, one can look at an old journal the "Econometrica" where @frischEditorsNote1933 states that "*Statistical and other numerical work presented in Econometrica, the original raw data will, as a rule, be published, unless their volume is excessive*".
This states that all the data ones used in its research should be presented if possible.
But as time passes, bigger models was needed to present the data which ended up in researchers only publicing the results and the reproducing or replicating of the research ended up nearly impossible.
Solutions had to be created for archiving the data, one was the American Economic Association data archive[@zotero-321].
@mccullough2008a points that "*All the long-standing archives at economics journals do not facilitate the re-production of published results. The data-only archives at Journal of business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data*".
With other words, the solution didn't work as one hoped for.
Another solution is the "computable documents" solution that uses codes integral in the article, so that the research can be submitted with the data necessary to fully be able to reproduce the research.
Through the use of codes one could reproduce research that used more and larger data then before.
One exempel on "computable documents" could be Wavelab which is a computer package that contains all the codes one require to reproduce figures from the researchers published wavelet articles.
This package comes from @WaveLabReproducibleResearch, where they want people that's interested to inspect the source codes so that scientist could be more engage in "really reproducible" research.
A quote from @gentleman2007b tells us how a software tool should be of assisten when it comes to reproducibility: "*The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the methods that are presented in the research paper*." This way the artical can contain the text, a code to read in the data, code to calculate the models and how to test them, and in the end, the code for the results on the research.
With that submitted in the journal, the research can then be fully reproducible.

So to implement these archiving of data, one would use programs such as Sweave how is the predecessor of Knitr, to blend codes and place the info in a Latex document[@SweaveDynamicGeneration].
The one we uses today is the Knitr which is supported by the use of Rstudio.
The Rstudio is a futher development from R notebook where we can run codes sequentially but at the same time each one can be run individually to experiment with the code-chunck[@landerEveryoneAdvancedAnalytics2017].
The program Rstudio structures the data, transforms it, visualize it and give out a model for the user.
Through the use of quarto documents in Rstudie, one can create a rapport that contains both the text and codes, use it to create documents in the format of HTML, PDF, word or slides.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].

## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?

So how is reproducibility in todays standard?

-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?

## Conclusion

## References

and

-   Version number and reference to packages used
-   R version used
